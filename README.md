# ğŸš€ Text Generation with RNN, GRU & Miniâ€‘GPT (Transformer)

> **An endâ€‘toâ€‘end NLP project that builds, analyzes, and compares classic sequence models (RNN, LSTM, GRU) with a Transformerâ€‘based Miniâ€‘GPT for wordâ€‘level text generation.**

This project is designed to **show real engineering maturity**:

* handling large text datasets safely
* understanding *why* certain models fail
* and upgrading to better architectures for the right reasons

---

## ğŸ“Œ Project Motivation

Text generation looks simple on the surface, but different deep learning models behave **very differently** when generating language.

This project answers three important questions:

1. â“ *Can RNNs really generate meaningful long text?*
2. â“ *Do GRU/LSTM fix repetition and semantic drift?*
3. â“ *Why do Transformers (GPTâ€‘style models) work so much better?*

Instead of jumping straight to Transformers, this project **intentionally starts with RNNs**, hits their limits, and then **moves to a Miniâ€‘GPT**.

---

## ğŸ§  Models Implemented

### 1ï¸âƒ£ RNN / LSTM (Baseline)

* Wordâ€‘level language model
* Nextâ€‘word prediction
* Demonstrates:

  * repetition issues
  * shortâ€‘term coherence
  * loss of meaning over long sequences

### 2ï¸âƒ£ GRU (Improved RNN)

* Fewer parameters than LSTM
* Faster convergence
* Reduced repetition
* Still suffers from **semantic drift**

### 3ï¸âƒ£ Miniâ€‘GPT (Transformer) â­

* Decoderâ€‘only Transformer
* Masked multiâ€‘head selfâ€‘attention
* Positional embeddings
* Maintains topic coherence
* Produces **significantly cleaner text**

---

## ğŸ§© Key Learning Outcome (IMPORTANT)

> This project proves that **text generation quality is limited by model architecture, not training tricks**.

Even with:

* more data
* more epochs
* sampling tricks

RNN/GRU models hit a **theoretical ceiling**.

Transformers break that ceiling using **selfâ€‘attention**.

---

## ğŸ“‚ Project Structure

```text
text_generation_rnn/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ raw/                # original text (ignored in git)
â”‚   â””â”€â”€ processed/          # tokenized & sequence data (ignored in git)
â”‚
â”œâ”€â”€ models/                 # trained models (ignored in git)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py       # dataset preprocessing
â”‚   â”œâ”€â”€ train_rnn.py        # RNN / LSTM training
â”‚   â”œâ”€â”€ train_gru.py        # GRU training
â”‚   â””â”€â”€ train_transformer.py# Miniâ€‘GPT training
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

> âš ï¸ Large datasets and trained weights are intentionally excluded from GitHub.

---

## ğŸ“Š Dataset

* **Source:** Wikipedia text corpus (Kaggle)
* **Type:** Plain text
* **Learning style:** Selfâ€‘supervised

### Preprocessing Highlights

* Wordâ€‘level tokenization
* Vocabulary limited to **20,000 words**
* Sliding window sequences
* Memoryâ€‘safe chunk loading (Colab friendly)

---

## âš™ï¸ Training Setup

| Component       | Value                           |
| --------------- | ------------------------------- |
| Vocabulary      | 20,000                          |
| Sequence length | 10â€“20                           |
| Batch size      | 128                             |
| Optimizer       | Adam                            |
| Loss            | Sparse Categorical Crossentropy |
| Platform        | Google Colab (GPU)              |

---

## âœ¨ Sample Outputs

### âŒ RNN / GRU Output (Expected Limitation)

```
deep learning is a field of artificial intelligence that the university of the journal of the prize of the university of
```

### âœ… Miniâ€‘GPT Output

```
deep learning is a field of artificial intelligence that enables systems to learn from data and improve performance without explicit programming
```

---

## ğŸ” Why Transformers Win

| Aspect             | RNN / GRU | Transformer |
| ------------------ | --------- | ----------- |
| Longâ€‘range context | âŒ         | âœ…           |
| Repetition control | âŒ         | âœ…           |
| Topic coherence    | âŒ         | âœ…           |
| Parallelism        | âŒ         | âœ…           |

---

## ğŸ› ï¸ Technologies Used

* **Python**
* **TensorFlow / Keras**
* **NumPy**
* **Google Colab (GPU)**
* **Git & GitHub**

---

## ğŸ§ª How to Run (Quick Start)

```bash
pip install tensorflow numpy
```

1. Add your text file to `dataset/raw/`
2. Run preprocessing
3. Train model of choice
4. Generate text from a seed prompt

---

## ğŸ§  What This Project Demonstrates

# âœ” Deep understanding of NLP pipelines
# âœ” Memoryâ€‘aware dataset handling
# âœ” Model comparison & failure analysis
# âœ” Correct use of Transformers
# âœ” Endâ€‘toâ€‘end ML engineering workflow

---


## ğŸš€ Future Work

* SentencePiece / BPE tokenization
* Characterâ€‘level vs subword comparison
* Fineâ€‘tuning preâ€‘trained GPT models
* Web demo using Streamlit

---

## ğŸ™Œ Final Note

This project was intentionally built **the hard way** â€” starting from RNNs â€” to truly understand *why* modern NLP uses Transformers.

